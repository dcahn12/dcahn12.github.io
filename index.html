<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Daechul Ahn</title>

  <meta name="author" content="Daechul Ahn">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;text-align:center"><tbody>
          <tr style="padding:0px">
            <td style="padding:0%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Daechul Ahn</name>
              </p>
              <p style="text-align:justify">
                I'm a Ph.D. student at
                  <a href="https://snumprlab.github.io/" target="_new">SNU Machine Perception and Reasoning Lab.</a>
                  at <a href="https://en.snu.ac.kr" target="_new">Seoul National University</a> advised by
                  Prof. <a href="http://ppolon.github.io/" target="_new">Jonghyun Choi</a>.
                My research focus is on multi-modal (specifically, vision-language) understanding and generation.
              </p>
              <p style="text-align:justify">
                During my M.S. at GIST, I was fortunate to work as a research intern at the <a href="https://www.lgresearch.ai" target="_new">LG AI Research</a> (2021-2022).
                Currently, I'm looking for an internship in vision-language research.
              </p>
              <p style="text-align:justify">
                I completed a Ph.D. course at <a href="https://www.yonsei.ac.kr" target="_new">Yonsei University</a>
                and received two M.S. degrees from <a href="https://www.gist.ac.kr" target="_new">Gwangju Institute of Science and Technology (GIST)</a>
                  under the supervision of Prof. <a href="http://ppolon.github.io/" target="_new">Jonghyun Choi</a>
                  and <a href="https://www.kaist.ac.kr" target="_new">Korea Institute of Science and Technology (KAIST)</a>
                  under the supervision of Prof. <a href="https://sites.google.com/view/nobelab/members/professor?authuser=0/" target="_new">Yang-Kyu Choi</a>
                  and a B.S. degree with first-class honours from <a href="https://www.ajou.ac.kr" target="_new">Ajou University</a>
                  under the supervision of Prof. <a href="https://scholar.google.co.kr/citations?user=6c38UAUAAAAJ&hl=ko" target="_new">Sangin Kim</a>.<br><br>

                  For additional details about past research and work experiences (e.g., semiconductor integration), please refer to the CV below!
              </p>
              <p style="text-align:center">
                <!--<a href="cv/cv.pdf" target="_new">CV</a> &nbsp/&nbsp-->
                <a href="https://drive.google.com/file/d/1nrSbUQUiUd2susRmt1PUZho4p1yFfldg/view?usp=share_link">CV</a> &nbsp/&nbsp
                <a href="mailto:daechulahn@snu.ac.kr">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1-IlIKEAAAAJ&hl=ko&oi=ao" target="_new">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Daechul-Ahn/2131876149" target="_new">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://dblp.org/pid/303/4574.html" target="_new">DBLP</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/daechul-ahn-a1b958145/" target="_new">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/dcahn12/" target="_new">Github</a>
              </p>
            </td>
            <td style="padding:0%;width:28%;max-width:40%">
              <img style="width:85%;max-width:100%" alt="profile photo" src="personal/dcahn_photo.jpeg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <br>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:0px;width:100%;vertical-align:middle"><heading>&nbsp&nbsp&nbsp Publications</heading></td>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;;style=text-align:justify"><tbody>

          <!-- ################################################################################################################### -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='papers/ahn2024vcmr/comingsoon.jpg' style="border-style: none width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle" align="justify">
              <a href="">
                <papertitle>
                    A paper about video corpus moment retrieval using moment-aware video retrieval 
                </papertitle>
              </a>
              <br>

              <a href="https://yuuraa.github.io/" target="_new"> Yura Choi* </a>,
              <strong> Daechul Ahn* </strong>,
              <a href="http://ppolon.github.io/" target="_new"> Jonghyun Choi </a>

              <br>
              Under Review 2024
              <br>

              <!-- [<a href="" target="_new">paper</a>] -->
              <!-- [<a href="ppapers/ahn2024srt/acl2024_rlaif.bib" target="_new">bibtex</a>]
              [<a href="https://github.com/snumprlab/SRT" target="_new">code</a>] -->
            </td>
          </tr>
          <!-- ################################################################################################################### -->

          <!-- ################################################################################################################### -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='papers/ahn2024srt/ahn2024srt.jpg' style="border-style: none width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle" align="justify">
              <a href="https://github.com/snumprlab/SRT" target="_new">
                <papertitle>
                    i-SRT: Aligning Large Multimodal Models for Videos by Iterative Self-Retrospective Judgment
                </papertitle>
              </a>
              <br>

              <strong> Daechul Ahn* </strong>,
              <a href="https://yuuraa.github.io/" target="_new"> Yura Choi* </a>,
              <a href=""> San Kim </a>,
              <a href="https://yj-yu.github.io/home/" target="_new"> Youngjae Yu </a>,
              <a href="https://dykang.github.io" target="_new"> Dongyeop Kang </a>,
              <a href="http://ppolon.github.io/" target="_new"> Jonghyun Choi </a>

              <br>
              arXiv 2024
              <br>

              [<a href="https://arxiv.org/pdf/2406.11280v1" target="_new">paper</a>]
              [<a href="ppapers/ahn2024srt/acl2024_rlaif.bib" target="_new">bibtex</a>]
              [<a href="https://github.com/snumprlab/SRT" target="_new">code</a>]
            </td>
          </tr>
          <!-- ################################################################################################################### -->


          <!-- ################################################################################################################### -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='papers/ahn2024rlaif/acl2024_rlaif.jpg' style="border-style: none width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle" align="justify">
              <a href="https://github.com/yonseivnl/vlm-rlaif" target="_new">
                <papertitle>
                    Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback
                </papertitle>
              </a>
              <br>

              <strong> Daechul Ahn </strong>,
              <a href="https://yuuraa.github.io/" target="_new"> Yura Choi </a>,
              <a href="https://yj-yu.github.io/home/" target="_new"> Youngjae Yu </a>,
              <a href="https://dykang.github.io" target="_new"> Dongyeop Kang </a>,
              <a href="http://ppolon.github.io/" target="_new"> Jonghyun Choi </a>

              <br>
              ACL 2024
              <br>

              [<a href="https://arxiv.org/pdf/2402.03746" target="_new">paper</a>]
              [<a href="papers/ahn2024rlaif/acl2024_rlaif.bib" target="_new">bibtex</a>]
              [<a href="https://github.com/yonseivnl/vlm-rlaif" target="_new">code</a>]
            </td>
          </tr>
          <!-- ################################################################################################################### -->


          <!-- ################################################################################################################### -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='papers/ahn2023cmota/iccv2023_cmota.jpg' style="border-style: none width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle" align="justify">
              <a href="https://dcahn12.github.io/projects/CMOTA" target="_new">
                <papertitle>
                    Story Visualization by Online Text Augmentation with Context Memory
                </papertitle>
              </a>
              <br>

              <strong> Daechul Ahn </strong>,
              <a href=""> Daneul Kim </a>,
              <a href=""> Gwangmo Song </a>,
              <a href=""> Seung Hwan Kim </a>,
              <a href="https://web.eecs.umich.edu/~honglak/" target="_new"> Honglak Lee </a>,
              <a href="https://dykang.github.io" target="_new"> Dongyeop Kang </a>,
              <a href="http://ppolon.github.io/" target="_new"> Jonghyun Choi </a>

              <br>
              ICCV 2023
              <br>

              [<a href="https://arxiv.org/abs/2308.07575" target="_new">paper</a>]
              [<a href="papers/ahn2023cmota/iccv2023_cmota.bib" target="_new">bibtex</a>]
              [<a href="https://github.com/yonseivnl/cmota" target="_new">code</a>]
            </td>
          </tr>
          <!-- ################################################################################################################### -->

          <!-- ################################################################################################################### -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='papers/nam2021psvl/iccv2021_zsnlvl.jpg' style="border-style: none width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle" align="justify">
              <a href="">
                <papertitle>
                    Zero-shot Natural Language Video Localization
                </papertitle>
              </a>
              <br>

              <a href=""> Jinwoo Nam* </a>,
              <strong> Daechul Ahn* </strong>,
              <a href="https://dykang.github.io" target="_new"> Dongyeop Kang </a>,
              <a href=""> Seong Jong Ha </a>,
              <a href="http://ppolon.github.io/" target="_new"> Jonghyun Choi </a>

              <br>
              ICCV 2021 (<span style="color: blue;"><strong>Oral, Acceptance rate: 3.2%</strong></span>)
              <br>

              [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Nam_Zero-Shot_Natural_Language_Video_Localization_ICCV_2021_paper.html" target="_new">paper</a>]
              [<a href="papers/nam2021psvl/iccv2021_zsnlvl.bib" target="_new">bibtex</a>]
              [<a href="https://github.com/gistvision/PSVL" target="_new">code</a>]
            </td>
          </tr>
        </tbody></table>

        <!-- Yonsei talk 1st at vision division -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:0px;width:100%;vertical-align:middle"><heading>&nbsp&nbsp&nbsp Awards</heading></td>
        </tbody></table>
        <ul>
            Computer vision Best Paper Award at the 1st Yonsei AI Workshop @ Oct 2022
        </ul>
        <br>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:0px;width:100%;vertical-align:middle"><heading>&nbsp&nbsp&nbsp Academic Services</heading></td>
        </tbody></table>
        <ul>
          <li><papertitle>Reviewer</papertitle></li>
          <ol style="margin-left: -3em">CVPR, ICCV, AAAI, WACV</ol>
        </ul>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This template is from <a href="https://jonbarron.info/" target="_new">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
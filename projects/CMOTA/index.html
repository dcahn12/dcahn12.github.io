<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Story Visualization by Online Text Augmentation with Context Memory">
  <meta name="keywords" content="Story visualization, Text-to-image generation, Online Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Story Visualization by Online Text Augmentation with Context Memory</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://dcahn12.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://bhkim94.github.io/projects/MOCA">
            MOCA
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/ABP">
            ABP
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/MCR-Agent">
            MCR-Agent
          </a>
          <a class="navbar-item" href="https://bhkim94.github.io/projects/CPEM">
            CPEM
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-full-width">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size:41px">
            Story Visualization by Online Text Augmentation with Context Memory
          </h1>
          <h1 class="title is-4 publication-title">
            ICCV 2023
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dcahn12.github.io">Daechul Ahn</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Daneul Kim</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Gwangmo Song</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Seung Hwan Kim</a><sup>3</sup>,
            </span>
            <br>
            <span class="author-block">
                <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a><sup>3,4</sup>,
            </span>
            <span class="author-block">
                <a href="https://dykang.github.io">Dongyeop Kang</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://ppolon.github.io">Jonghyun Choi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Yonsei University,</span>
            <span class="author-block"><sup>2</sup>Gwangju Institute of Science and Technology</span>
            <span class="author-block"><sup>3</sup>LG AI Research</span>
            <br>
            <span class="author-block"><sup>4</sup>University of Michigan</span>
            <span class="author-block"><sup>5</sup>University of Minnesota</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.07575"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>-->
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-4 is-centered has-text-centered">üèÜ Challenge Winners üèÜ</h2>
      <h2 class="title is-5 is-centered has-text-centered"> <a href="https://askforalfred.com/EAI23/">1st Generalist Language Grounding Agents Challenge (CVPRW'23)</a> </h2>
      <br>
      <img src='static/figures/teaser.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:65%">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Context-aware Planning and Environment-aware Memory (CPEM)</span>
      </h2>
    </div>
  </div>
</section> -->



<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts
                mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video.
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>-->
      <!--/ Paper video. -->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Context Memory with Attentively Weighted </h2>
    <!--
    <div class="columns is-centered">
      <div class="column is-two-quarters">
        <div class="content has-text-justified">
          <p>
            dd
          </p>
        </div>
      </div>
      <div class="column is-two-quarter">
        <img src='static/figures/cp.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:70%">
      </div>
    </div>
    -->
    <div class="content has-text-justified">
      <p>
        Perception models of the current embodied AI agents, however, often make mistakes due to a lack of such knowledge but rely on imperfect learning of imitating agents or an algorithmic planner without knowledge about the changed environment by the previous actions.
        To address the issue, we propose CPEM to incorporate <span style="font-weight:bold">the contextual information</span> of previous actions for planning and maintaining spatial arrangement of objects <span style="font-weight:bold">with their states</span> (e.g., if an object has been moved or not) in an environment to the perception model for improving both visual navigation and object interaction.
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/overview.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:70%">
        </div>
      </div>
    </div>


    <h3 class="title is-4">Context-aware Planning</h3>
    <div class="columns is-centered">
      <div class="column is-two-quarters">
        <div class="content has-text-justified">
          <p>
            To plan a sequence of sub-goals conditioned on the task-relevant objects, we first define `context' as a set of task-relevant objects shared across sub-goals of a given task.
            The proposed `context-aware planning' (CP) divides planning into two phases; 1) a <span style="font-weight:bold">sub-goal planner</span> which generates sub-goals, and 2) a <span style="font-weight:bold">detailed planner</span> which is responsible for a sequence of detailed actions and objects for interaction for each sub-goal.
          </p>
          <p>
            The sub-goal planner further comprises two sub-modules: the <span style="font-weight:bold">context predictor</span>, which predicts three task-relevant objects, and the <span style="font-weight:bold">sub-goal frame sequence generator</span>, which generates a sequence of sub-goals that do not rely on particular objects, referred to as sub-goal frames.
            We integrate these predicted task-specific objects into a sequence of sub-goal frames to produce a sub-goal sequence.
          </p>
        </div>
      </div>
      <div class="column is-two-quarter">
        <img src='static/figures/cp.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
      </div>
    </div>
    <!--
    <div class="content has-text-justified">
      <p>
        dd
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/cp.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:70%">
        </div>
      </div>
    </div>
    -->

    <h3 class="title is-4">Environment-aware Memory</h3>
    <div class="columns is-centered">
      <div class="column is-two-quarters">
        <div class="content has-text-justified">
          <p>
            To enable the agent to keep track of the configurations of objects, we propose to configure memory of past environmental information for proper action sequence prediction during a task.
          </p>
          <p>
            <span style="font-weight:bold">Retrospective Object Recognition.</span>
            To allow the agent to keep interacting with the same object even with the visual appearance changes during multiple interactions, we propose to retain the latest segmentation masks of objects and use them as the current object's mask if the agent is interacting with the same object but fails to recognize it.
          </p>
          <p>
            <span style="font-weight:bold">Object Relocation Tracking.</span>
            To allow the agent to avoid redundant interaction with already relocated objects, we propose to maintain the information about the most recent location of each relocated object and exclude them in the semantic map as a future target for navigation.
          </p>
          <p>
            <span style="font-weight:bold">Object Location Caching.</span>
            To reduce the need for agents to explore an environment again, we propose to cache the locations and the masks in memory for objects whose states change such that the agent can navigate back to and interact with the remembered locations and object masks.
          </p>
        </div>
      </div>
      <div class="column is-two-quarter">
        <img src='static/figures/em.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
      </div>
    </div>
    <!--
    <div class="content has-text-justified">
      <p>
        dd
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/em.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:70%">
        </div>
      </div>
    </div>
  </div>
  -->
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
            Following previous works, we compare the performance with prior arts in Pororo-SV and Flintstones-SV dataset on the various metrics, i.e., FID, Character-F1 (Char.F1), Frame Accuracy (Frm.Acc), BLEU-2/3 and R-precision (R-Prc.), and resolutions, i.e., 64 √ó 64 and 128 x 128. 
            CMOTA shows overall performance improvement, thereby outperforming existing methods by a large margin in both benchmark datasets.
            CMOTA-HR outperforms CMOTA with low-resolution (i.e., 64 √ó 64) in almost all metrics. 
            However, VLC-HR  drops in all metrics compared to VLC with low-resolution. 
            Also, in terms of computational cost, our CMOTA gains performance boost by using high-resolution effectively with just adding relatively small computational cost on top of original 64 √ó 64 model.
        </p>
        <p>
            Comparing VLC-StoryGAN-HR with VLC-StoryGAN in 64 √ó 64, we see that HR model shows similar performance to 64 √ó 64 model in Frm.Acc, BLEU, R-prec. 
            As our CMOTA has superior ability in generating small characters and objects with accurate depiction of background, our CMOTA-HR model shows better score in all metrics except in FID.
            We can observe the performance drop of FID in both our CMOTA and VLC-StoryGAN. As high-resolution image requires the model to depict fine-grained details, generation task would be more difficult compared to the low-resolution image generation. 
            Therefore, we believe that the abrupt drop of FID in Flintstones-SV is due to the complicated visual details compared to Pororo-SV dataset. 
            Nevertheless, the FID drops less with CMOTA than the VLC-StoryGAN in Pororo-SV.
        </p>
        <p>
            The very recent method VP-CSV performs on par to our CMOTA while ours still outperforms in FID, BLEU and R-precision, implying that CMOTA generates high quality images maintaining global semantic matching between story paragraph and images better than the VP-CSV. 
            The lower performance than the VP-CSV in Char. F1 score and Frm.
            Acc. is attributed to the specialized character-centric module that only focuses the model to generate accurate characters for each image at the expense of the other performance metrics.
        </p>
        <p>
          For more details, please check out the <span style="color: #305fac; font-weight:bold"><a href="https://arxiv.org/abs/2308.07575" target="_new">paper</a></span>.
        </p>
        <br>

        <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
            <img src="static/tables/main_table.jpeg" width="900">
            <h5>Comparison with Prior Arts</h5>
        </div>
        </div> -->
        
        <div class="columns is-centered has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/tables/main_table.jpeg" width="800">
                <h5>Quantitative Comparison with State-of-the-Art</h5>
              </div>
            </div>
  
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/figures/per_char_cla_f1.jpg" width="550">
                <h5>Per-character Classification F1-score on the Test Split of Pororo-SV Dataset</h5>
              </div>
            </div>
        </div>

        <div class="columns is-centered has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/figures/comparison_prior_arts.jpg" width="600">
                <h5>Qualitative Comparison with State-of-the-Art</h5>
              </div>
            </div>
  
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/figures/memory_efficacy.jpg" width="550">
                <h5>Benefit of Memory Module in Encoding Background Context</h5>
              </div>
            </div>
        </div>


      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{ahn2023cmota,
  author    = {Ahn, Daechul and Kim, Daneul and Song, Gwangmo and Kim, Seung Hwan and Lee, Honglak and Kang, Dongyeop and Choi, Jonghyun},
  title     = {Story Visualization by Online Text Augmentation with Context Memory},
  booktitle = {ICCV},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The website template is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Story Visualization by Online Text Augmentation with Context Memory">
  <meta name="keywords" content="Story visualization, Text-to-image generation, Online Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Story Visualization by Online Text Augmentation with Context Memory</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-full-width">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size:41px">
            Story Visualization by Online Text Augmentation with Context Memory
          </h1>
          <h1 class="title is-4 publication-title">
            ICCV 2023
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dcahn12.github.io">Daechul Ahn</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Daneul Kim</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Gwangmo Song</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Seung Hwan Kim</a><sup>3</sup>,
            </span>
            <br>
            <span class="author-block">
                <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a><sup>3,4</sup>,
            </span>
            <span class="author-block">
                <a href="https://dykang.github.io">Dongyeop Kang</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://ppolon.github.io">Jonghyun Choi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Yonsei University,</span>
            <span class="author-block"><sup>2</sup>Gwangju Institute of Science and Technology</span>
            <span class="author-block"><sup>3</sup>LG AI Research</span>
            <br>
            <span class="author-block"><sup>4</sup>University of Michigan</span>
            <span class="author-block"><sup>5</sup>University of Minnesota</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ahn_Story_Visualization_by_Online_Text_Augmentation_with_Context_Memory_ICCV_2023_paper.pdf" target="_new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.07575" target="_new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/yonseivnl/cmota" target="_new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>-->
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. 
                While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. 
                To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. 
                We call our model as <span style="font-weight:bold">C</span>ontext <span style="font-weight:bold">M</span>emory and <span style="font-weight:bold">O</span>nline <span style="font-weight:bold">T</span>ext <span style="font-weight:bold">A</span>ugmentation or <span style="font-weight:bold">CMOTA</span> for short.
                In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.
            </p>
            <!-- <br> -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-fullwidth">
                <img src='static/figures/teaser.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:85%">
                <h5>Context encoding by 'Context Memory' and Robust to liguistic diversity by 'Online Text-Augmentation'</h5>
              </div>
            </div>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video.
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>-->
      <!--/ Paper video. -->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Context Memory with Attentively Weighted Connection</h2>
    <div class="content has-text-justified">
      <p>
        Story visualization task is challenging for the requirement of rendering the visual details in images with convincing background of a scene – seasonal elements, environmental objects such as table, location, and the proper character appearing, which here we refer to as <span style="font-weight:bold">context</span>, spread across the given text sentences. 
        To better encode such semantic context, e.g., plausible background and characters, we propose a new memory architecture, call it as <span style="font-weight:bold">context memory</span>.
        In particular, we utilize transformer architecture with explicitly connecting last layers for propagating historical information.
        Additionally, not all historic information is equally important for generating an image at a time step. Similar to the masked self-attention, we attentively weight the past information for better modeling of sparse context, i.e., <span style="font-weight:bold">attentively weighted memory</span>.
        By utilizing this, we fuse the contextual memory information into current state, thereby generating temporally coherent and semantically relevant sequence.
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/context_memory.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:60%">
        </div>
      </div>
    </div>

  </div>

  <br><br>

  <div class="container is-max-widescreen">
    <h2 class="title is-3">Online Text-Augmentation</h2>
      <p>
        We further propose to generate pseudo-texts during the learning process to augment (online-augmentation) for better linguistic generalization without requiring large external data by learning the bi-directional Transformers in both directions of generating images from texts and vice versa.
      </p>
      <br>
      <h3 class="title is-4">Bi-directional Transformer</h3>
      <div class="columns is-centered">
        <div class="column is-two-quarters">
          <div class="content has-text-justified">
            <p>
              We use bi-directional (i.e., multi-modal) transformer that iteratively generates images and texts in an unified architecture. 
              Similar to DALL-E (Ramesh et al., ICML'21), the image tokens are sequentially predicted from the input text sequences by the Transformer. 
              Then the decoder of the VQ-VAE translates the predicted image tokens into image sequence. 
              The text tokens are also sequentially predicted from the input image token sequence by the same Transformer. 
              Particularly, for the bi-directional multi-modal generation, we add two embeddings; a positional embedding for absolute position between tokens and a segment embedding for distinguishing source and target modality.
            </p>
          </div>
        </div>
        <div class="column is-two-quarter">
          <img src='static/figures/bidirectional_tr.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:90%;max-width:80%">
        </div>
      </div>

      <h3 class="title is-4">Online Text-Augmentation</h3>
      <div class="columns is-centered">
        <div class="column is-two-quarters">
          <div class="content has-text-justified">
            <p>
              Thanks to our bi-directional Transformers architecture, we can naturally integrate the process of generating pseudo-texts to the process of learning image-to-text model and the text-to-image generation model. 
              In the early epochs, less meaningful sentences are generated, but as training progresses, more meaningful sentences are generated. 
              As a side-product, by supervising the model learning with intermediate goals at each time step, we expect to expedite the convergence of learning.
            </p>
          </div>
        </div>
        <div class="column is-two-quarter">
          <img src='static/figures/ota_simple.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div>
      </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
          Following previous works, we compare the performance with prior arts in Pororo-SV and Flintstones-SV dataset on the various metrics, i.e., FID, Character-F1 (Char.F1), Frame Accuracy (Frm.Acc), BLEU-2/3 and R-precision (R-Prc.), and resolutions, i.e., 64 × 64 and 128 x 128.
          CMOTA shows overall performance improvement, thereby outperforming existing methods by a large margin in both benchmark datasets.
          Also, our CMOTA generates qualitatively more plausible image sequence with better visual quality, compared to prior works.
          Further, we observe the advatage of using memory architecture.
          With the context memory, our CMOTA generates a semantically more plausible image sequence with proper context, e.g., background, while the CMOTA without memory fails to capture proper background context since a single sentence could be interpreted in many ways.
        </p>
        <p>
          For more details, please check out the <span style="color: #305fac; font-weight:bold"><a href="https://arxiv.org/abs/2308.07575" target="_new">paper</a></span>.
        </p>
        <br>

        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <img src="static/tables/main_table.jpeg" width="1000">
            <h5>Quantitative Comparison with State-of-the-Art</h5>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/figures/comparison_prior_arts.jpg" width="500">
                <h5>Qualitative Comparison with State-of-the-Art</h5>
              </div>
            </div>
  
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/figures/memory_efficacy.jpg" width="500">
                <h5>Benefit of Memory Module in Encoding Background</h5>
              </div>
            </div>
        </div>


      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{ahn2023cmota,
  author    = {Ahn, Daechul and Kim, Daneul and Song, Gwangmo and Kim, Seung Hwan and Lee, Honglak and Kang, Dongyeop and Choi, Jonghyun},
  title     = {Story Visualization by Online Text Augmentation with Context Memory},
  booktitle = {ICCV},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The website template is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
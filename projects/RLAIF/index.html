<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback">
  <meta name="keywords" content="Multimodal Alignment, Video Multimodal Large Model, Reinforcement Learning from AI Feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-full-width">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size:41px">
            Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback
          </h1>
          <h1 class="title is-4 publication-title">
            ACL 2024
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dcahn12.github.io" target="_new">Daechul Ahn</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://yuuraa.github.io" target="_new">Yura Choi</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://yj-yu.github.io/home/" target="_new">Youngjae Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://dykang.github.io" target="_new">Dongyeop Kang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ppolon.github.io">Jonghyun Choi</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Yonsei University,</span>
            <span class="author-block"><sup>5</sup>University of Minnesota</span>
            <span class="author-block"><sup>5</sup>Seoul National University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2402.03746" target="_new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/yonseivnl/vlm-rlaif" target="_new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>-->
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). 
                Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. 
                Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient quality and quantity of multimodal instruction-tune data compared to that of text-only. 
                This discrepancy often results in alignments that poorly ground the video content. 
                To address this, we present a novel alignment strategy that employs a multimodal AI system equipped with Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. 
                Our approach uniquely integrates detailed video descriptions as context into a multimodal AI system during preference feedback generation to enrich the understanding of video content, a process we call context-aware reward modeling. 
                Empirical evaluations on various video benchmarks demonstrate that our proposed method outperforms existing approaches, including the SFT model.
            </p>
            <!-- <br> -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-fullwidth">
                <img src='static/figures/rlaif_feedback_teaser.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
                <h5>Illustration of the proposed VLM-RLAIF</h5>
              </div>
            </div>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video.
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>-->
      <!--/ Paper video. -->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">VLM-RLAIF Framework</h2>
    <div class="content has-text-justified">
      <p>
        Story visualization task is challenging for the requirement of rendering the visual details in images with convincing background of a scene â€“ seasonal elements, environmental objects such as table, location, and the proper character appearing, which here we refer to as <span style="font-weight:bold">context</span>, spread across the given text sentences. 
        To better encode such semantic context, e.g., plausible background and characters, we propose a new memory architecture, call it as <span style="font-weight:bold">context memory</span>.
        In particular, we utilize transformer architecture with explicitly connecting last layers for propagating historical information.
        Additionally, not all historic information is equally important for generating an image at a time step. Similar to the masked self-attention, we attentively weight the past information for better modeling of sparse context, i.e., <span style="font-weight:bold">attentively weighted memory</span>.
        By utilizing this, we fuse the contextual memory information into current state, thereby generating temporally coherent and semantically relevant sequence.
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/context_memory.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:60%">
        </div>
      </div>
    </div>

  </div>

  <br><br>

  <div class="container is-max-widescreen">
    <h2 class="title is-3">Online Text-Augmentation</h2>
      <p>
        We further propose to generate pseudo-texts during the learning process to augment (online-augmentation) for better linguistic generalization without requiring large external data by learning the bi-directional Transformers in both directions of generating images from texts and vice versa.
      </p>
      <br>
      <h3 class="title is-4">Bi-directional Transformer</h3>
      <div class="columns is-centered">
        <div class="column is-two-quarters">
          <div class="content has-text-justified">
            <p>
              We use bi-directional (i.e., multi-modal) transformer that iteratively generates images and texts in an unified architecture. 
              Similar to DALL-E (Ramesh et al., ICML'21), the image tokens are sequentially predicted from the input text sequences by the Transformer. 
              Then the decoder of the VQ-VAE translates the predicted image tokens into image sequence. 
              The text tokens are also sequentially predicted from the input image token sequence by the same Transformer. 
              Particularly, for the bi-directional multi-modal generation, we add two embeddings; a positional embedding for absolute position between tokens and a segment embedding for distinguishing source and target modality.
            </p>
          </div>
        </div>
        <div class="column is-two-quarter">
          <img src='static/figures/bidirectional_tr.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:90%;max-width:80%">
        </div>
      </div>

      <h3 class="title is-4">Online Text-Augmentation</h3>
      <div class="columns is-centered">
        <div class="column is-two-quarters">
          <div class="content has-text-justified">
            <p>
              Thanks to our bi-directional Transformers architecture, we can naturally integrate the process of generating pseudo-texts to the process of learning image-to-text model and the text-to-image generation model. 
              In the early epochs, less meaningful sentences are generated, but as training progresses, more meaningful sentences are generated. 
              As a side-product, by supervising the model learning with intermediate goals at each time step, we expect to expedite the convergence of learning.
            </p>
          </div>
        </div>
        <div class="column is-two-quarter">
          <img src='static/figures/ota_simple.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div>
      </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
          Following previous works, we compare the performance with prior arts in Pororo-SV and Flintstones-SV dataset on the various metrics, i.e., FID, Character-F1 (Char.F1), Frame Accuracy (Frm.Acc), BLEU-2/3 and R-precision (R-Prc.), and resolutions, i.e., 64 Ã— 64 and 128 x 128.
          CMOTA shows overall performance improvement, thereby outperforming existing methods by a large margin in both benchmark datasets.
          Also, our CMOTA generates qualitatively more plausible image sequence with better visual quality, compared to prior works.
          Further, we observe the advatage of using memory architecture.
          With the context memory, our CMOTA generates a semantically more plausible image sequence with proper context, e.g., background, while the CMOTA without memory fails to capture proper background context since a single sentence could be interpreted in many ways.
        </p>
        <p>
          For more details, please check out the <span style="color: #305fac; font-weight:bold"><a href="https://arxiv.org/abs/2308.07575" target="_new">paper</a></span>.
        </p>
        <br>

        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <img src="static/tables/main_table.jpeg" width="1000">
            <h5>Quantitative Comparison with State-of-the-Art</h5>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/figures/comparison_prior_arts.jpg" width="500">
                <h5>Qualitative Comparison with State-of-the-Art</h5>
              </div>
            </div>
  
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img src="static/figures/memory_efficacy.jpg" width="500">
                <h5>Benefit of Memory Module in Encoding Background</h5>
              </div>
            </div>
        </div>


      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{ahnCYKC24,
  author    = {Ahn, Daechul and Choi, Yura and Yu, Youngjae and Kang, Dongyeop and Choi, Jonghyun},
  title     = {Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback},
  booktitle = {ACL},
  year      = {2024},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The website template is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback">
  <meta name="keywords" content="Multimodal Alignment, Video Multimodal Large Model, Reinforcement Learning from AI Feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-full-width">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size:41px">
            Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback
          </h1>
          <h1 class="title is-4 publication-title">
            ACL 2024
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dcahn12.github.io" target="_new">Daechul Ahn</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://yuuraa.github.io" target="_new">Yura Choi</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://yj-yu.github.io/home/" target="_new">Youngjae Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://dykang.github.io" target="_new">Dongyeop Kang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ppolon.github.io">Jonghyun Choi</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Yonsei University,</span>
            <span class="author-block"><sup>2</sup>University of Minnesota</span>
            <span class="author-block"><sup>3</sup>Seoul National University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2402.03746" target="_new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/yonseivnl/vlm-rlaif" target="_new"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>-->
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). 
                Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. 
                Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient quality and quantity of multimodal instruction-tune data compared to that of text-only. 
                This discrepancy often results in alignments that poorly ground the video content. 
                To address this, we present a novel alignment strategy that employs a multimodal AI system equipped with Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. 
                Our approach uniquely integrates detailed video descriptions as context into a multimodal AI system during preference feedback generation to enrich the understanding of video content, a process we call context-aware reward modeling. 
                Empirical evaluations on various video benchmarks demonstrate that our proposed method outperforms existing approaches, including the SFT model.
            </p>
            <!-- <br> -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-fullwidth">
                <img src='static/figures/radar.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:60%">
                <h5>Quantitative comparison of VLMMs on various video benchmarks</h5>
              </div>
            </div>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video.
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>-->
      <!--/ Paper video. -->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-widescreen">
    <h1 class="title is-3">VLM-RLAIF Framework</h1>
    <div class="content has-text-justified">
      <p>
        To overcome the limited scalability of human feedback in RLHF, we use AI’s feedback to align multimodality between video and text, reducing the reliance on exhaustive human-annotated preferences. 
        The training procedure of VLM-RLAIF can be summarized into three stages as follows:
        <h5 class="title is-5">1) Supervised fine-tuning (SFT)</h5>
          <p>
            We first fine-tune an LLM, e.g., Vicuna, using supervised learning on synthetically generated video-text instruction-tune data. 
            This involves the integration of a vision encoder with two linear layers and additional learnable parameters using LoRA, into the training process. 
            This fine-tuning allows the model to better follow the instructions.
            Additionally, we improve the SFT process by expanding the instruction-tune data and introducing simple curriculum learning. 
            We refer to this fine-tuned model as the Video Large Multimodal model with SFT or <span style="font-weight:bold">VLM-SFT</span> for short.
          </p>
        <h5 class="title is-5">2) Reward modeling with AI feedback</h5>
          <p>
            A key aspect of the RLAIF involves leveraging a pre-trained AI model to generate human-like preferences between different responses generated from the same input.
            To obtain human-like preference, we employ the VLM-SFT as a judge to assess preferences. 
            Once preferences are judged, we train a reward model (RM) based on preferences using a cross-entropy loss, following the Bradley-Terry model for estimating score functions from pairwise preferences.
            The RM give higher score reward to the better response and lower score reward to the less appropriate one in a pair of responses, thus guiding the policy model using reinforcement learning.
          </p>
        <h5 class="title is-5">3) Reinforcement learning from AI feedback</h5>
          <p>
            We finally fine-tune a supervised policy model, initialized from the VLM-SFT, aiming to optimize the scalar reward output of the trained RM by reinforcement learning. 
            Specifically, we use the Proximal Policy Optimization (PPO) algorithm.
          </p>
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src='static/figures/rlaif_feedback_teaser.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
          <h5>Illustration of the proposed VLM-RLAIF</h5>
        </div>
      </div>
    </div>

  </div>

  <br><br>

  <div class="container is-max-widescreen">
    <h3 class="title is-4">Context-Aware Reward Modeling</h3>
      <p>
        For VLM-SFT to select preference grounded on the video, we argue that a <span style="font-weight:bold">detailed understanding of video content</span> is necessary for more accurate and contextually relevant decisions by the VLM-SFT. 
        However, the current video encoder presents challenges in accurately encoding the temporal details of videos as they are based on the image encoder.
      </p>
      <br>
      <!-- <h3 class="title is-4">Context-aware preference selection and Training the reward model</h3> -->
      <div class="columns is-centered">
        <div class="column is-two-quarter">
          <div class="content has-text-justified">
            <p>
              We propose integrating detailed video descriptions, termed as <span style="font-weight:bold; font-style: italic;">context</span>, into the preference selection workflow to enhance VLMM's contextual clarity. 
              The process involves segmenting the video into clips of up to 20 frames each, generating detailed descriptions for each segment using VLM-SFT with the prompt, ‘Describe this video in detail’. 
              These descriptions are concatenated to form a narrative of the video, which is then provided to a judge model (VLM-SFT) for improved preference selection. 
              This context allows the VLM-SFT to better understand the video content and identify the most suitable response. 
              Integrating context with instruction inputs using a specific prompt, as shown in dotted boxes in the right Figure (2), facilitates the collection of context-aware preferences.
            </p>
            <p>
              The right figure illustrates the three stages of the proposed context-aware reward modeling:
              (1) The SFT model produces two candidate responses from the provided video and question. 
              (2) With the video, question and responses at hand, the SFT model utilize context information and guiding prompt to evaluate the responses. 
              (3) The reward model is trained using the preference pairs generated in the previous step as indicated in orange box. 
              Each stage involves a model's input, indicated by a dotted box, and includes a task-specific prompt, shown in a yellow box. The first stage focuses on response generation, the second on evaluation and selection of the superior response, and the third on training the reward model.
            </p>
          </div>
        </div>
        <div class="column is-fullwidth">
          <img src='static/figures/reward_modeling.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:85%">
          <span style="display: block; text-align: center; width: 100%; font-weight: bold;">Three stages of the context-aware reward modeling</span>
        </div>
        <!-- <div class="column is-two-quarter">
          <img src='static/figures/reward_modeling.png' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div> -->
      </div>

      <!-- <h3 class="title is-4">Online Text-Augmentation</h3>
      <div class="columns is-centered">
        <div class="column is-two-quarters">
          <div class="content has-text-justified">
            <p>
              Thanks to our bi-directional Transformers architecture, we can naturally integrate the process of generating pseudo-texts to the process of learning image-to-text model and the text-to-image generation model. 
              In the early epochs, less meaningful sentences are generated, but as training progresses, more meaningful sentences are generated. 
              As a side-product, by supervising the model learning with intermediate goals at each time step, we expect to expedite the convergence of learning.
            </p>
          </div>
        </div>
        <div class="column is-two-quarter">
          <img src='static/figures/ota_simple.jpg' style="margin-left: auto; margin-right: auto; display: block; border-style: none width:100%;max-width:100%">
        </div>
      </div> -->
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
          We quantitatively evaluate VLMMs on the video-based generative performance benchmark that measures five criteria of generated text. 
          In specific, these assess the relevance of the model’s output to the video content, its capacity to capture essential details and contextual information, its understanding of temporal sequences, and the consistency in responding to varied yet related queries. 
          The VLM-RLAIF performs on par with GPT-4V (OpenAI, 2023), which requires much more computational resources than ours (i.e., not a fair comparison), and outperforms previous approaches and the VLM-SFT.
        </p>
        <p>
          Moreover, we qualitatively compares the performance of VLM-SFT and VLM-RLAIF, highlighting their multimodal understanding capabilities below Figure. 
          VLM-RLAIF consistently yields more accurate answers than VLM-SFT, 
          The VLM-RLAIF yields responses that are more accurately grounded in the visual input, as highlighted in <span style="color: blue;">blue</span> for accurate responses and <span style="color: red;">red</span> for less accurate ones from VLM-SFT.
        </p>
        <p>
          For more information, please check out the <span style="color: #305fac; font-weight:bold"><a href="https://arxiv.org/pdf/2402.03746" target="_new">paper</a></span>.
        </p>
        <br>

        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <img src="static/tables/rlaif_result.jpg" width="1200">
            <h5>Quantitative comparison between different VLMMs on video-based generative performance benchmark</h5>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <img src="static/figures/sft_rlaif.png" width="1100">
            <h5>Qualitative examples of the comparative results between VLM-SFT and VLM-RLAIF in video instruction-following task</h5>
          </div>
        </div>


      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{ahnCYKC24,
  author    = {Ahn, Daechul and Choi, Yura and Yu, Youngjae and Kang, Dongyeop and Choi, Jonghyun},
  title     = {Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback},
  booktitle = {ACL},
  year      = {2024},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          The website template is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>